---
title: "R Notebook"
output: html_notebook
---

### By Jack Wilburn under the supervision of Jaimi Butler

Reference materials at: [TensorFlow](https://tensorflow.rstudio.com/blog/keras-image-classification-on-small-datasets.html) and [Deep Learning with R](https://livebook.manning.com/#!/book/deep-learning-with-r/chapter-5/9)

### Libraries

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(lubridate)
library(imager)
library(stringi)
library(reshape)
library(keras)
library(jpeg)
library(magick)
```

### Import

```{r, message=FALSE, warning=FALSE}
# Import hand classification
train = read_csv("train_clean.csv")

# List all images (for running on new files)
files <- paste("PeliPhotos1Folder/", list.files("PeliPhotos1Folder/", pattern = "\\.jpg$"), sep = "")

train$filelong <- paste("PeliPhotos1Folder/", train$file, sep = "")

# Import images 500 are 10GB
list_of_images <- lapply(train$filelong[1:506], load.image)
# Crop to the beach
list_of_images <- lapply(list_of_images, imsub, x > 600, y > 300)
# Grayscale them, 500 are 3.4
list_of_images <- lapply(list_of_images, grayscale)

# Import images 500 are 10GB
list_of_images2 <- lapply(train$filelong[507:1011], load.image)
# Crop to the beach
list_of_images2 <- lapply(list_of_images2, imsub, x > 600, y > 300)
# Grayscale them, 500 are 3.4
list_of_images2 <- lapply(list_of_images2, grayscale)

images <- c(list_of_images, list_of_images2)
rm(list_of_images, list_of_images2)

# cannyEdges may give 50% reduction in size

# For loop to unpack list into matrix
array <- array(dim = c(1011, 420, 680))
i = 0

while (i < length(images)) {
  image <- images[[i+1]]
  j = 0
  while (j < 420) {
    k = 0
    while (k < 680) {
      array[i,j,k] = image[k,j,1,1]
      k = k + 1
    }
    j =j + 1
  }
  i = i+1
}

rm(images, image, i, j, k, files)
```

### Converting Book Code To New Application

```{r}
set.seed(1)
subset <- sample(1011, 506)
train_images <- array[subset, , ]
train_labels <- ifelse(train$pelicans[subset], 1, 0)
train_labels[is.na(train_labels)] <- 0
test_images <- array[-subset, , ]
test_labels <- ifelse(train$pelicans[-subset], 1, 0)
test_labels[is.na(test_labels)] <- 0

train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
rm(train, array, subset)

train_images <- array_reshape(train_images, c(506, 420 * 680))
test_images <- array_reshape(test_images, c(505, 420 * 680))

network <- keras_model_sequential() %>% 
  layer_dense(units = 1000, activation = "relu", input_shape = c(420 * 680)) %>%
  layer_dense(units = 1000, activation = "relu", input_shape = c(420 * 680)) %>%
  layer_dense(units = 2, activation = "sigmoid")

network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("categorical_accuracy"))

network %>% fit(train_images, train_labels, epochs = 5, batch_size = 32)

metrics <- network %>% evaluate(test_images, test_labels, verbose = 0)
metrics
```

### Book code
```{r}
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y

network <- keras_model_sequential() %>% 
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>% 
  layer_dense(units = 10, activation = "softmax")

network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255

train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)

network %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)

metrics <- network %>% evaluate(test_images, test_labels, verbose = 0)
metrics



#Convnets
model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(28, 28, 1)) %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu")

model <- model %>% 
  layer_flatten() %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 10, activation = "softmax")

mnist <- dataset_mnist()
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist
train_images <- array_reshape(train_images, c(60000, 28, 28, 1))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28, 28, 1))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)

model %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

model %>% fit(
  train_images, train_labels, 
  epochs = 5, batch_size=64
)

results <- model %>% evaluate(test_images, test_labels)
results
```

